{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'initial_summary': {'num_records': 583, 'num_columns': 25}, 'final_summary': {'num_records': 583, 'num_columns': 26}, 'output_file_path': 'outputs/project/modified_output.json'}\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = 'outputs/project'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to fetch data from local file (CSV or JSON)\n",
    "def fetch_data(source, file_type='csv'):\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            return pd.read_csv(source)\n",
    "        elif file_type == 'json':\n",
    "            with open(source, 'r') as file:\n",
    "                return pd.DataFrame(json.load(file))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert data between CSV and JSON\n",
    "def convert_data(df, output_format='csv'):\n",
    "    try:\n",
    "        if output_format == 'csv':\n",
    "            return df.to_csv(index=False)\n",
    "        elif output_format == 'json':\n",
    "            return df.to_json(orient='records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in converting data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to add/remove columns from the dataset\n",
    "def modify_columns(df, columns_to_add=None, columns_to_remove=None):\n",
    "    try:\n",
    "        if columns_to_add:\n",
    "            for col_name, col_data in columns_to_add.items():\n",
    "                df[col_name] = col_data\n",
    "        if columns_to_remove:\n",
    "            df.drop(columns=columns_to_remove, inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in modifying columns: {e}\")\n",
    "        return None\n",
    "\n",
    "#4.  Function to store data in SQL database or locally as a file\n",
    "def store_data(df, output_file=None, sql_db=None, table_name=None):\n",
    "    try:\n",
    "        if output_file:\n",
    "            if output_file.endswith('.csv'):\n",
    "                df.to_csv(output_file, index=False)\n",
    "            elif output_file.endswith('.json'):\n",
    "                with open(output_file, 'w') as file:\n",
    "                    json.dump(json.loads(df.to_json(orient='records')), file)\n",
    "        if sql_db:\n",
    "            conn = sqlite3.connect(sql_db)\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in storing data: {e}\")\n",
    "\n",
    "# Function to generate summary of data\n",
    "def generate_summary(df):\n",
    "    try:\n",
    "        summary = {\n",
    "            'num_records': len(df),\n",
    "            'num_columns': len(df.columns)\n",
    "        }\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generating summary: {e}\")\n",
    "        return None\n",
    "\n",
    "# 1. Ask the user for the input file path and format\n",
    "input_file_path = input(\"Enter the path of the input file (CSV/JSON): \")\n",
    "input_file_type = input(\"Enter the input file format (csv/json): \").lower()\n",
    "\n",
    "# Fetch the input data\n",
    "df = fetch_data(input_file_path, file_type=input_file_type)\n",
    "\n",
    "# Generating initial summary\n",
    "initial_summary = generate_summary(df)\n",
    "\n",
    "# 3. Modifying the dataset (example: adding a new column)\n",
    "if df is not None:\n",
    "    modified_df = modify_columns(df, columns_to_add={'New_Col': [1]*len(df)})\n",
    "\n",
    "# Generating final summary after modifications\n",
    "final_summary = generate_summary(modified_df)\n",
    "\n",
    "# 2. Ask the user for desired output format (CSV or JSON)\n",
    "output_format = input(\"Enter the desired output file format (csv/json): \").lower()\n",
    "\n",
    "# 4. storeCheck if conversion is necessary and convert the data if required\n",
    "if input_file_type != output_format:\n",
    "    # Convert the data to the desired output format\n",
    "    converted_data = convert_data(modified_df, output_format)\n",
    "    \n",
    "    # Saving the converted data\n",
    "    output_file_path = os.path.join(output_dir, f'modified_output.{output_format}')\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        file.write(converted_data)\n",
    "else:\n",
    "    # If no conversion is needed, just save it in the same format\n",
    "    output_file_path = os.path.join(output_dir, f'modified_output.{output_format}')\n",
    "    store_data(modified_df, output_file=output_file_path)\n",
    "\n",
    "# Output the final summaries\n",
    "print({\n",
    "    'initial_summary': initial_summary,\n",
    "    'final_summary': final_summary,\n",
    "    'output_file_path': output_file_path\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "The goal of this project was to implement an ETL (Extract, Transform, Load) data processor capable of ingesting either CSV or JSON files, allowing users to modify the data, and converting the data between these formats as per the user’s requirements. The final data could be stored locally as a file in the desired format, as well as optionally saved to an SQLite database.\n",
    "\n",
    "Dynamic Input and Output Handling: One of the biggest challenges was dynamically handling different file formats (CSV/JSON). This required creating flexible functions to fetch and store data, as well as ensuring smooth conversion between formats. Ensuring that both CSV and JSON input could be processed correctly, and then converting between them, introduced complexity in the data manipulation logic.\n",
    "\n",
    "Handling Edge Cases: Another challenge involved dealing with edge cases, such as malformed JSON files, missing columns in CSVs, or large datasets. It was crucial to provide meaningful error messages and ensure that the code could handle such cases gracefully without crashing.\n",
    "\n",
    "User-Friendly Design: Designing the code to be user-friendly required a balance between functionality and simplicity. The user needed to provide input and output formats and file paths without being overwhelmed by technical details. Ensuring that the user interactions were clear and concise took careful planning.\n",
    "\n",
    "What Was Easier Than Expected:\n",
    "Using Pandas for Data Manipulation: Pandas proved to be a powerful tool for reading, modifying, and writing data. Its ability to seamlessly handle both CSV and JSON data formats made the development process more straightforward. Once the data was loaded into a Pandas DataFrame, modifying and converting it was relatively easy.\n",
    "\n",
    "Directory Management and File Handling: Managing file paths and directories for storing outputs was simpler than anticipated. Python’s os.makedirs function ensured that output directories were created automatically if they did not exist, reducing manual setup.\n",
    "\n",
    "What Was Harder Than Expected:\n",
    "Handling Different Data Types for SQLite: Storing JSON-like structures into an SQLite database presented challenges. SQLite doesn't support certain complex data types (like lists or dictionaries), so converting these structures into supported formats (e.g., strings) before storing them required additional logic.\n",
    "\n",
    "Error Handling and Validation: Ensuring proper validation of user input (e.g., checking if the file path is valid or if the format is correct) and implementing robust error handling across various scenarios took more effort. It was important to anticipate possible failures and provide informative feedback to users.\n",
    "\n",
    "Usefulness for Future Data Projects:\n",
    "This ETL utility is a versatile tool that can be incredibly useful in future data projects for the following reasons:\n",
    "\n",
    "Flexible Data Ingestion: The ability to ingest data from different formats (CSV or JSON) and convert between them makes it useful for projects requiring data integration from multiple sources.\n",
    "Data Transformation: The utility allows easy manipulation of the dataset (e.g., adding/removing columns), which is a common requirement in data cleaning and transformation tasks.\n",
    "Automation of Repetitive Tasks: This utility automates the process of loading, transforming, and storing data, reducing manual effort. It can be extended to work with different file formats, databases, or even APIs.\n",
    "Streamlined Workflow: By enabling flexible output formats and allowing modifications, this tool streamlines the data preprocessing pipeline, making it easier to integrate into larger data workflows.\n",
    "\n",
    "Overall, this project provided valuable insights into building a flexible, user-friendly ETL pipeline. The challenges faced reinforced the importance of robust error handling and flexibility when dealing with diverse data sources and formats. This utility has broad applications in many data-related tasks, including data cleaning, transformation, and integration in future projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
